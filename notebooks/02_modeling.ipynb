{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbde52b",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "# Model Development\n",
    "\n",
    "This notebook develops and tunes the machine learning models for:\n",
    "- Anomaly Detection\n",
    "- Sentiment Analysis\n",
    "- Risk Scoring\n",
    "- Trend Analysis\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64055dba",
   "metadata": {},
   "source": [
    "### 1. Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a84edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path.cwd().parent / 'src'))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fc6d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.anomaly_detector import HybridAnomalyDetector\n",
    "from models.sentiment_engine import SentimentAnalyzer\n",
    "from models.risk_scorer import BayesianRiskScorer\n",
    "from models.trend_analyzer import TrendAnalyzer\n",
    "from preprocessing.feature_extractor import AdvancedFeatureExtractor\n",
    "\n",
    "print(\"âœ“ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea27006",
   "metadata": {},
   "source": [
    "### 2. Generate Synthetic Training Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb6f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(n_samples=1000):\n",
    "    \"\"\"Generate synthetic data for model training\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Normal data\n",
    "    X_normal = np.random.randn(int(n_samples * 0.9), 10)\n",
    "    \n",
    "    # Anomalies (10%)\n",
    "    X_anomalies = np.random.randn(int(n_samples * 0.1), 10) * 3 + 5\n",
    "    \n",
    "    # Combine\n",
    "    X = np.vstack([X_normal, X_anomalies])\n",
    "    y = np.hstack([np.zeros(len(X_normal)), np.ones(len(X_anomalies))])\n",
    "    \n",
    "    # Shuffle\n",
    "    indices = np.random.permutation(len(X))\n",
    "    X, y = X[indices], y[indices]\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "X_train, y_train = generate_training_data()\n",
    "print(f\"âœ“ Generated {len(X_train)} training samples\")\n",
    "print(f\"  Normal: {(y_train == 0).sum()}\")\n",
    "print(f\"  Anomalies: {(y_train == 1).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6d306",
   "metadata": {},
   "source": [
    "### 3. Anomaly Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136c318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ANOMALY DETECTOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Initialize and train\n",
    "detector = HybridAnomalyDetector(contamination=0.1, n_estimators=100)\n",
    "detector.fit(X_train)\n",
    "\n",
    "# Predict\n",
    "predictions = detector.predict(X_train)\n",
    "scores = detector.predict(X_train, return_scores=True)\n",
    "\n",
    "print(f\"\\nâœ“ Model trained\")\n",
    "print(f\"Detected anomalies: {predictions.sum()}\")\n",
    "print(f\"Score range: [{scores.min():.3f}, {scores.max():.3f}]\")\n",
    "\n",
    "# Visualize scores\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c87a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score distribution\n",
    "axes[0].hist(scores, bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(np.percentile(scores, 90), color='red', linestyle='--', \n",
    "                label='90th percentile (threshold)')\n",
    "axes[0].set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Anomaly Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127cb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores over samples\n",
    "axes[1].scatter(range(len(scores)), scores, c=predictions, \n",
    "                cmap='RdYlGn_r', alpha=0.6, s=20)\n",
    "axes[1].set_title('Anomaly Scores Across Samples', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sample Index')\n",
    "axes[1].set_ylabel('Anomaly Score')\n",
    "axes[1].axhline(np.percentile(scores, 90), color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path.cwd().parent / 'data' / 'processed' / 'anomaly_scores.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90c0bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "if detector.feature_importance_ is not None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    feature_names = [f'Feature {i}' for i in range(len(detector.feature_importance_))]\n",
    "    importance_sorted = sorted(zip(feature_names, detector.feature_importance_), \n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    features, importances = zip(*importance_sorted[:10])\n",
    "    \n",
    "    ax.barh(range(len(features)), importances, color='coral')\n",
    "    ax.set_yticks(range(len(features)))\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.set_xlabel('Importance')\n",
    "    ax.set_title('Top 10 Feature Importances', fontsize=14, fontweight='bold')\n",
    "    ax.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ“ Feature importance calculated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8010ee0e",
   "metadata": {},
   "source": [
    "### 4. Sentiment Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c74520e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SENTIMENT ANALYZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b0c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING SENTIMENT ANALYZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "analyzer = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe4a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\nSentiment Analysis Results:\")\n",
    "print(\"-\" * 80)\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Text: {result['text']}\")\n",
    "    print(f\"   Label: {result['label'].upper()}\")\n",
    "    print(f\"   Confidence: {result['confidence']:.2%}\")\n",
    "    print(f\"   Scores: Pos={result['scores']['positive']:.2f}, \"\n",
    "          f\"Neu={result['scores']['neutral']:.2f}, Neg={result['scores']['negative']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9835bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "labels = [r['label'] for r in results]\n",
    "label_counts = pd.Series(labels).value_counts()\n",
    "\n",
    "colors = {'positive': '#10B981', 'neutral': '#6B7280', 'negative': '#EF4444'}\n",
    "bar_colors = [colors.get(label, 'gray') for label in label_counts.index]\n",
    "\n",
    "ax.bar(label_counts.index, label_counts.values, color=bar_colors, edgecolor='black', linewidth=2)\n",
    "ax.set_title('Sentiment Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Sentiment')\n",
    "ax.set_ylabel('Count')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Sentiment analysis tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea299ed",
   "metadata": {},
   "source": [
    "### 5. Risk Scoring Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdda1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING RISK SCORER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "risk_scorer = BayesianRiskScorer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d68dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'High Risk Scenario',\n",
    "        'sentiment_score': -0.8,\n",
    "        'sentiment_volatility': 0.6,\n",
    "        'time_series_values': np.random.randn(100) * 3 + 10,\n",
    "        'trend_slope': -0.5,\n",
    "        'trend_strength': 0.9,\n",
    "        'anomaly_score': 0.85,\n",
    "        'source_credibility': 0.9,\n",
    "        'timestamp': pd.Timestamp.now()\n",
    "    },\n",
    "    {\n",
    "        'name': 'Low Risk Scenario',\n",
    "        'sentiment_score': 0.6,\n",
    "        'sentiment_volatility': 0.2,\n",
    "        'time_series_values': np.random.randn(100) * 0.5 + 5,\n",
    "        'trend_slope': 0.2,\n",
    "        'trend_strength': 0.7,\n",
    "        'anomaly_score': 0.15,\n",
    "        'source_credibility': 0.95,\n",
    "        'timestamp': pd.Timestamp.now()\n",
    "    },\n",
    "    {\n",
    "        'name': 'Medium Risk Scenario',\n",
    "        'sentiment_score': -0.3,\n",
    "        'sentiment_volatility': 0.4,\n",
    "        'time_series_values': np.random.randn(100) * 2 + 7,\n",
    "        'trend_slope': -0.2,\n",
    "        'trend_strength': 0.5,\n",
    "        'anomaly_score': 0.5,\n",
    "        'source_credibility': 0.8,\n",
    "        'timestamp': pd.Timestamp.now()\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c770e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assess each scenario\n",
    "assessments = []\n",
    "for scenario in scenarios:\n",
    "    assessment = risk_scorer.assess_risk(scenario)\n",
    "    assessments.append({\n",
    "        'name': scenario['name'],\n",
    "        'score': assessment.overall_score,\n",
    "        'level': assessment.risk_level.value,\n",
    "        'confidence': assessment.confidence\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{scenario['name']}:\")\n",
    "    print(f\"  Risk Score: {assessment.overall_score:.2%}\")\n",
    "    print(f\"  Risk Level: {assessment.risk_level.value.upper()}\")\n",
    "    print(f\"  Confidence: {assessment.confidence:.2%}\")\n",
    "    print(f\"  Top Recommendation: {assessment.recommendations[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bb5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize risk assessments\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "names = [a['name'] for a in assessments]\n",
    "scores = [a['score'] for a in assessments]\n",
    "colors_map = {'critical': '#EF4444', 'high': '#F59E0B', 'medium': '#F59E0B', 'low': '#10B981'}\n",
    "bar_colors = [colors_map.get(a['level'], 'gray') for a in assessments]\n",
    "\n",
    "bars = ax.barh(names, scores, color=bar_colors, edgecolor='black', linewidth=2)\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_xlabel('Risk Score')\n",
    "ax.set_title('Risk Assessment Comparison', fontsize=14, fontweight='bold')\n",
    "ax.axvline(0.7, color='red', linestyle='--', alpha=0.5, label='High Risk Threshold')\n",
    "ax.axvline(0.4, color='orange', linestyle='--', alpha=0.5, label='Medium Risk Threshold')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add value labels\n",
    "for i, (bar, score) in enumerate(zip(bars, scores)):\n",
    "    ax.text(score + 0.02, i, f'{score:.1%}', va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path.cwd().parent / 'data' / 'processed' / 'risk_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Risk scoring tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7475a3",
   "metadata": {},
   "source": [
    "### 6. Trend Analysis Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a615886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TESTING TREND ANALYZER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "trend_analyzer = TrendAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abda1a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate time series with trend\n",
    "t = np.arange(100)\n",
    "trend_component = 0.5 * t\n",
    "seasonal_component = 10 * np.sin(2 * np.pi * t / 7)\n",
    "noise = np.random.randn(100) * 3\n",
    "values = trend_component + seasonal_component + noise + 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b541c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect trend\n",
    "trend_info = trend_analyzer.detect_trend(values)\n",
    "print(f\"\\nTrend Detection:\")\n",
    "print(f\"  Direction: {trend_info['direction']}\")\n",
    "print(f\"  Slope: {trend_info['slope']:.3f}\")\n",
    "print(f\"  Strength (RÂ²): {trend_info['r_squared']:.3f}\")\n",
    "print(f\"  P-value: {trend_info['p_value']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b3703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition\n",
    "decomp = trend_analyzer.decompose_time_series(values, period=7)\n",
    "print(f\"\\nTime Series Decomposition:\")\n",
    "print(f\"  Trend strength: {decomp['trend_strength']:.3f}\")\n",
    "print(f\"  Seasonal strength: {decomp['seasonal_strength']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a3d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "forecast_result = trend_analyzer.forecast_simple(values, periods=14)\n",
    "print(f\"\\n14-Period Forecast:\")\n",
    "print(f\"  Method: {forecast_result['method']}\")\n",
    "print(f\"  Next values: {forecast_result['forecast'][:3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f87d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize trend analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original series\n",
    "axes[0, 0].plot(values, linewidth=2, color='darkblue', label='Original')\n",
    "axes[0, 0].set_title('Original Time Series', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].set_ylabel('Value')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Trend component\n",
    "axes[0, 1].plot(decomp['trend'], linewidth=2, color='red', label='Trend')\n",
    "axes[0, 1].set_title('Extracted Trend', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Time')\n",
    "axes[0, 1].set_ylabel('Value')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Seasonal component\n",
    "axes[1, 0].plot(decomp['seasonal'], linewidth=2, color='green', label='Seasonal')\n",
    "axes[1, 0].set_title('Seasonal Component', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].set_ylabel('Value')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Forecast\n",
    "forecast_x = np.arange(len(values), len(values) + len(forecast_result['forecast']))\n",
    "axes[1, 1].plot(values, linewidth=2, color='darkblue', label='Historical', alpha=0.7)\n",
    "axes[1, 1].plot(forecast_x, forecast_result['forecast'], linewidth=2, \n",
    "                color='red', linestyle='--', label='Forecast')\n",
    "axes[1, 1].fill_between(forecast_x, \n",
    "                         forecast_result['lower_bound'], \n",
    "                         forecast_result['upper_bound'],\n",
    "                         alpha=0.3, color='red', label='95% CI')\n",
    "axes[1, 1].set_title('Forecast with Confidence Interval', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].set_ylabel('Value')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(Path.cwd().parent / 'data' / 'processed' / 'trend_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ“ Trend analysis tested\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a531fc69",
   "metadata": {},
   "source": [
    "### 7. Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b79152",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL DEVELOPMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "summary = {\n",
    "    'anomaly_detection': {\n",
    "        'algorithm': 'Hybrid Ensemble (Isolation Forest + Density + PCA)',\n",
    "        'contamination': 0.1,\n",
    "        'n_estimators': 100,\n",
    "        'score_range': [float(scores.min()), float(scores.max())],\n",
    "        'detected_anomalies': int(predictions.sum())\n",
    "    },\n",
    "    'sentiment_analysis': {\n",
    "        'model': 'Rule-based with lexicon',\n",
    "        'supported_languages': ['English', 'Sinhala', 'Tamil'],\n",
    "        'output': 'Positive/Neutral/Negative + Confidence'\n",
    "    },\n",
    "    'risk_scoring': {\n",
    "        'method': 'Bayesian with Beta-Binomial conjugate prior',\n",
    "        'components': 6,\n",
    "        'output': 'Score (0-1) + Level + Recommendations'\n",
    "    },\n",
    "    'trend_analysis': {\n",
    "        'methods': ['Linear regression', 'Decomposition', 'Forecasting'],\n",
    "        'forecast_horizon': '14 periods',\n",
    "        'includes_confidence_interval': True\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "summary_path = Path.cwd().parent / 'data' / 'processed' / 'model_summary.json'\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nModel Components:\")\n",
    "print(\"  âœ“ Anomaly Detection: Hybrid ensemble\")\n",
    "print(\"  âœ“ Sentiment Analysis: Multi-lingual\")\n",
    "print(\"  âœ“ Risk Scoring: Bayesian framework\")\n",
    "print(\"  âœ“ Trend Analysis: Decomposition + Forecast\")\n",
    "print(f\"\\nSummary saved to: {summary_path}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485cd1a8",
   "metadata": {},
   "source": [
    "### 8. Save Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f71395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import MODELS_DIR\n",
    "\n",
    "# Save anomaly detector\n",
    "detector.save(MODELS_DIR / 'anomaly_detector.pkl')\n",
    "print(f\"âœ“ Saved anomaly detector to: {MODELS_DIR / 'anomaly_detector.pkl'}\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Model development complete! Ready for evaluation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
